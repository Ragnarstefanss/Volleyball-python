{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import codecs, json\n",
    "import unicodedata\n",
    "# pip install Unidecode  <OR> conda install Unidecode\n",
    "import unidecode\n",
    "import collections\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables and what they mean\n",
    "#   duplicate_dict = all duplicates\n",
    "#   dict_removed_single_entries = all duplicates but removed single entries\n",
    "#   duplicate_ids_kept = array of all ids from dict_removed_single_entries\n",
    "#   dict_duplicate_compare_team_members = map values from teams to einstaklingsid\n",
    "#   dict_name_entries = map values from member down one step (name->birthday->values) now (name+birthday->values)\n",
    "#   dict_einstaklingar_teammember_info = map teammember values to correct key in einstaklingsid\n",
    "#   not_the_same_person = when two players are playing in different teams at the same time then they clearly are not the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all csv files\n",
    "domarar = pd.read_csv('csv/blak-domarar.csv', sep=';', header=0)\n",
    "einstaklingar = pd.read_csv('csv/blak-einstaklingar.csv', sep=';', header=0)\n",
    "forsvarsmenn = pd.read_csv('csv/blak-forsvarsmenn.csv', sep=';', header=0)\n",
    "lid = pd.read_csv('csv/blak-lid.csv', sep=';', header=0)\n",
    "lidimoti = pd.read_csv('csv/blak-lidimoti.csv', sep=';', header=0)\n",
    "lidsmenn = pd.read_csv('csv/blak-lidsmenn.csv', sep=';', header=0)\n",
    "lidsstjorar = pd.read_csv('csv/blak-lidsstjorar.csv', sep=';', header=0)\n",
    "thjalfarar = pd.read_csv('csv/blak-thjalfarar.csv', sep=';', header=0)\n",
    "mot = pd.read_csv('csv/blak-mot.csv', sep=';', header=0)\n",
    "\n",
    "# drop all SyndarLids with an ID (SyndarlidID)\n",
    "# (the reason for not dropping using SyndarLid is because I don't trust that column to be inserted correctly with [0,1])\n",
    "lid = lid[lid['SyndarlidID'].isna()]\n",
    "# then dropping those two columns because we don't want virtual teams\n",
    "lid = lid.drop(columns=['SyndarLid', 'SyndarlidID'])\n",
    "\n",
    "# All duplicated birthdays\n",
    "duplicated_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Nafn', 'Fdagur', 'Kyn'], keep=False)]\n",
    "duplicated_fdagur_kyn_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Fdagur', 'Kyn'], keep=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter duplicates by name and birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all entries that have duplicated birthdays, then filter that to first_name->birthday-><people entries>\n",
    "duplicate_dict = defaultdict(dict)\n",
    "for index, row in duplicated_fdagur_kyn_einstaklingar.iterrows():\n",
    "    full_name = row['Nafn']\n",
    "    #only get the first part of full name \n",
    "    first_name = full_name.split()[0]\n",
    "    # make first name lowercase\n",
    "    first_name_lowercase = first_name.lower()\n",
    "    # encode icelandic letters to english\n",
    "    first_name_to_english = unidecode.unidecode(first_name_lowercase)\n",
    "    # split birthday into year month and day and ignore second part (sec, min, hour)\n",
    "    Fdagur_date = row['Fdagur'].split()[0]\n",
    "    \n",
    "    if first_name_to_english in duplicate_dict.keys():\n",
    "        if Fdagur_date in duplicate_dict[first_name_to_english].keys():\n",
    "            #if first name and Fdagur (birthday) exist in dict then append to that key (birthday)\n",
    "            duplicate_dict[first_name_to_english][Fdagur_date].append(row.values)\n",
    "        else:\n",
    "            #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "            duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "    else:\n",
    "        #if Fdagur (birthday) does not exist in dict\n",
    "        duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all single birthday entries (since that is not a duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all single birthday entries that are not duplicates\n",
    "dict_removed_single_entries = defaultdict(dict)\n",
    "\n",
    "for key, values in duplicate_dict.items():\n",
    "    # key = nafn ('ludvik')\n",
    "    for birthday, arrays in dict(values).items():\n",
    "        # only get duplicates that there exists 2 or more entries for a birthday\n",
    "        if(len(arrays) > 1):\n",
    "            # used for when joining teams table\n",
    "            if key in dict_removed_single_entries.keys():\n",
    "                if birthday in dict_removed_single_entries[key].keys():\n",
    "                    dict_removed_single_entries[key][birthday].append(arrays)\n",
    "                else:\n",
    "                    #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "                    dict_removed_single_entries[key][birthday] = arrays\n",
    "            else:\n",
    "                dict_removed_single_entries[key][birthday] = arrays\n",
    "\n",
    "#dict_removed_single_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all ids that exists in duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ids in dict_removed_single_entries\n",
    "duplicate_ids_kept = []\n",
    "for key, values in dict_removed_single_entries.items():\n",
    "    # key = nafn ('ludvik')\n",
    "    for birthday, arrays in dict(values).items():\n",
    "        for item in arrays:\n",
    "            duplicate_ids_kept.append(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map teams table values to einstaklingsID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if two names are the same person\n",
    "dict_duplicate_compare_team_members = defaultdict(dict)\n",
    "for index, row in lidsmenn.iterrows():\n",
    "    ids = row[\"EinstID\"]\n",
    "    if ids in duplicate_ids_kept:\n",
    "        # now we only view ids that exist for duplicated people\n",
    "        #print(ids)\n",
    "        if ids in dict_duplicate_compare_team_members.keys():\n",
    "            dict_duplicate_compare_team_members[ids].append(row.values)\n",
    "        else:\n",
    "            dict_duplicate_compare_team_members[ids] = [row.values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map one step down (name->birthday -> value) now (name+birthday -> value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name_entries = {}\n",
    "for key, value in dict_removed_single_entries.items():\n",
    "    #get key and arrays for each person\n",
    "    for birthday, arrays in dict(value).items():\n",
    "        #get each array for person\n",
    "        #print(\"KEY: \" + key + \" BIRTHDAY: \" + birthday)\n",
    "        new_key = key +\"-\"+ birthday\n",
    "        for item in arrays:\n",
    "            if new_key in dict_name_entries.keys():\n",
    "                dict_name_entries[new_key].append(item[0])\n",
    "            else:\n",
    "                dict_name_entries[new_key] = [item[0]]\n",
    "#dict_name_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EinstaklingsID+birthday connected to all his data from teams table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_einstaklingar_teammember_info = {}\n",
    "for key, value in dict_name_entries.items():\n",
    "    #print(\"<key>\" + str(key) + \" <value> \" + str(value))\n",
    "    for item in value:\n",
    "        #print(item)\n",
    "        if item in dict_duplicate_compare_team_members.keys():\n",
    "            #print(\"<key>\" + str(key) + \" <item> \" + str(item))\n",
    "            for compare_arrays in dict_duplicate_compare_team_members[item]:\n",
    "                mot_id = compare_arrays[0]\n",
    "                lid_id = compare_arrays[1]\n",
    "                player_id = compare_arrays[2]\n",
    "                date_played = compare_arrays[3].split()[0]\n",
    "                \n",
    "                temp = (str(compare_arrays[3]) + \" \" + str(player_id) + \" \" + str(lid_id))   \n",
    "                if key in dict_einstaklingar_teammember_info.keys():\n",
    "                    dict_einstaklingar_teammember_info[key].append(temp)\n",
    "                else:\n",
    "                    dict_einstaklingar_teammember_info[key] = [temp]\n",
    "        #print(\"xxxxxxxxxxxx\")\n",
    "#dict_einstaklingar_teammember_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find if a potential duplicated person played two games at the same time in different teams (then he is not a duplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Time 1: 2012-04-11 13:50:36.313000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"datetime.datetime\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-da462bed8728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_einstaklingar_teammember_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#print(value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mfind_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-da462bed8728>\u001b[0m in \u001b[0;36mfind_duplicates\u001b[0;34m(key, nums)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=======================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time 1: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_time_obj_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time 2: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdate_time_obj_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Key: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Row 1: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"datetime.datetime\") to str"
     ]
    }
   ],
   "source": [
    "not_the_same_person = {}\n",
    "\n",
    "def getTimeDifferenceFromNow(TimeStart, TimeEnd):\n",
    "    timeDiff = TimeEnd - TimeStart\n",
    "    return timeDiff.total_seconds() / 60\n",
    "\n",
    "def find_duplicates(key, nums):\n",
    "    num_set = set()\n",
    "    duplicates = set()\n",
    "    no_duplicate = -1\n",
    "    sorted_nums = sorted(nums)\n",
    "    \n",
    "    for i in range(len(sorted_nums)):\n",
    "        for j in range(i+1, len(sorted_nums)):\n",
    "            \n",
    "            # team one split\n",
    "            sort_1 = sorted_nums[i].split()\n",
    "            date_1 = sort_1[0]\n",
    "            einstaklings_id_1 = sort_1[2]\n",
    "            team_id_1 = sort_1[3]\n",
    "            date_time_str_1 = sort_1[0]+\" \"+sort_1[1]\n",
    "            date_time_obj_1 = datetime.datetime.strptime(date_time_str_1, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # team two split\n",
    "            sort_2 = sorted_nums[j].split()\n",
    "            date_2 = sort_2[0]\n",
    "            einstaklings_id_2 = sort_2[2]\n",
    "            team_id_2 = sort_2[3]            \n",
    "            date_time_str_2 = sort_1[0]+\" \"+sort_1[1]\n",
    "            date_time_obj_2 = datetime.datetime.strptime(date_time_str_2, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            \n",
    "            \n",
    "            \n",
    "            #time_diff = (time_2_final - time_1_final).time()\n",
    "            \n",
    "            if((date_1 == date_2) and (team_id_1 != team_id_2)):\n",
    "                print(\"=======================\")\n",
    "                print(\"Time 1: \" + str(date_time_obj_1))\n",
    "                print(\"Time 2: \" + str(date_time_obj_2))\n",
    "                print(\"Key: \" + key)\n",
    "                print(\"Row 1: \")\n",
    "                print(sorted_nums[i])\n",
    "                print(\"Row 2: \")\n",
    "                print(sorted_nums[j]);\n",
    "                print(\"END OF THESE ROWS\")\n",
    "                \n",
    "    \n",
    "for key, value in dict_einstaklingar_teammember_info.items():\n",
    "    #print(value)\n",
    "    find_duplicates(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================================================================\n",
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverted_back_to_dict = dict(duplicate_dict)\n",
    "#reverted_back_to_dict\n",
    "#file_path = \"json/einstaklingar_map.txt\" ## your path variable\n",
    "#duplicate_dict_json = json.dump(duplicate_dict, codecs.open(file_path, 'w', encoding='utf-8'), separators=(';', ':'), sort_keys=True, indent=4) ### this saves the array in .json format\n",
    "#json_obj = json.dumps(duplicate_dict, indent = 4)\n",
    "#dumped = json.dumps(duplicate_dict, cls=NumpyEncoder)\n",
    "#dumped\n",
    "#pd.DataFrame(reverted_back_to_dict).to_csv(file_path, encoding='utf-8-sig')\n",
    "#duplicate_dict_json = json.dump(reverted_back_to_dict, codecs.open(file_path, 'w', encoding='utf-8-sig'))\n",
    "\n",
    "#json = json.dumps(reverted_back_to_dict)\n",
    "#f = open(file_path,\"w\")\n",
    "#f.write(str(reverted_back_to_dict))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================================================================\n",
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL STEP (run after everything is done):\n",
    "\n",
    "#duplicated people put into it's own csv to be browsed later\n",
    "pd.DataFrame(duplicated_einstaklingar).to_csv(\"csv/new/duplicated-einstaklingar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(duplicate_dict).to_csv(\"json/duplicate-map.json\", encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "#save as new csv inside csv/new\n",
    "pd.DataFrame(domarar).to_csv(\"csv/new/blak-domarar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(einstaklingar).to_csv(\"csv/new/blak-einstaklingar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(forsvarsmenn).to_csv(\"csv/new/blak-forsvarsmenn.csv.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lid).to_csv(\"csv/new/blak-lid.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidimoti).to_csv(\"csv/new/blak-lidimoti.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsmenn).to_csv(\"csv/new/blak-lidsmenn.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsstjorar).to_csv(\"csv/new/blak-lidsstjorar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(mot).to_csv(\"csv/new/blak-mot.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(thjalfarar).to_csv(\"csv/new/blak-thjalfarar.csv\", encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
