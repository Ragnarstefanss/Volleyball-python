{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import codecs, json\n",
    "import unicodedata\n",
    "# pip install Unidecode  <OR> conda install Unidecode\n",
    "import unidecode\n",
    "import collections\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables and what they mean\n",
    "#   duplicate_dict = all duplicates\n",
    "#   dict_removed_single_entries = all duplicates but removed single entries\n",
    "#   duplicate_ids_kept = array of all ids from dict_removed_single_entries\n",
    "#   dict_duplicate_compare_team_members = map values from teams to einstaklingsid\n",
    "#   dict_name_entries = map values from member down one step (name->birthday->values) now (name+birthday->values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all csv files\n",
    "domarar = pd.read_csv('csv/blak-domarar.csv', sep=';', header=0)\n",
    "einstaklingar = pd.read_csv('csv/blak-einstaklingar.csv', sep=';', header=0)\n",
    "forsvarsmenn = pd.read_csv('csv/blak-forsvarsmenn.csv', sep=';', header=0)\n",
    "lid = pd.read_csv('csv/blak-lid.csv', sep=';', header=0)\n",
    "lidimoti = pd.read_csv('csv/blak-lidimoti.csv', sep=';', header=0)\n",
    "lidsmenn = pd.read_csv('csv/blak-lidsmenn.csv', sep=';', header=0)\n",
    "lidsstjorar = pd.read_csv('csv/blak-lidsstjorar.csv', sep=';', header=0)\n",
    "thjalfarar = pd.read_csv('csv/blak-thjalfarar.csv', sep=';', header=0)\n",
    "mot = pd.read_csv('csv/blak-mot.csv', sep=';', header=0)\n",
    "\n",
    "# drop all SyndarLids with an ID (SyndarlidID)\n",
    "# (the reason for not dropping using SyndarLid is because I don't trust that column to be inserted correctly with [0,1])\n",
    "lid = lid[lid['SyndarlidID'].isna()]\n",
    "# then dropping those two columns because we don't want virtual teams\n",
    "lid = lid.drop(columns=['SyndarLid', 'SyndarlidID'])\n",
    "\n",
    "# All duplicated birthdays\n",
    "duplicated_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Nafn', 'Fdagur', 'Kyn'], keep=False)]\n",
    "duplicated_fdagur_kyn_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Fdagur', 'Kyn'], keep=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter duplicates by name and birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all entries that have duplicated birthdays, then filter that to first_name->birthday-><people entries>\n",
    "duplicate_dict = defaultdict(dict)\n",
    "for index, row in duplicated_fdagur_kyn_einstaklingar.iterrows():\n",
    "    full_name = row['Nafn']\n",
    "    #only get the first part of full name \n",
    "    first_name = full_name.split()[0]\n",
    "    # make first name lowercase\n",
    "    first_name_lowercase = first_name.lower()\n",
    "    # encode icelandic letters to english\n",
    "    first_name_to_english = unidecode.unidecode(first_name_lowercase)\n",
    "    # split birthday into year month and day and ignore second part (sec, min, hour)\n",
    "    Fdagur_date = row['Fdagur'].split()[0]\n",
    "    \n",
    "    if first_name_to_english in duplicate_dict.keys():\n",
    "        if Fdagur_date in duplicate_dict[first_name_to_english].keys():\n",
    "            #if first name and Fdagur (birthday) exist in dict then append to that key (birthday)\n",
    "            duplicate_dict[first_name_to_english][Fdagur_date].append(row.values)\n",
    "        else:\n",
    "            #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "            duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "    else:\n",
    "        #if Fdagur (birthday) does not exist in dict\n",
    "        duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all single birthday entries (since that is not a duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all single birthday entries that are not duplicates\n",
    "dict_removed_single_entries = defaultdict(dict)\n",
    "\n",
    "for key, values in duplicate_dict.items():\n",
    "    # key = nafn ('ludvik')\n",
    "    for birthday, arrays in dict(values).items():\n",
    "        # only get duplicates that there exists 2 or more entries for a birthday\n",
    "        if(len(arrays) > 1):\n",
    "            # used for when joining teams table\n",
    "            if key in dict_removed_single_entries.keys():\n",
    "                if birthday in dict_removed_single_entries[key].keys():\n",
    "                    dict_removed_single_entries[key][birthday].append(arrays)\n",
    "                else:\n",
    "                    #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "                    dict_removed_single_entries[key][birthday] = arrays\n",
    "            else:\n",
    "                dict_removed_single_entries[key][birthday] = arrays\n",
    "\n",
    "#dict_removed_single_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all ids that exists in duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ids in dict_removed_single_entries\n",
    "duplicate_ids_kept = []\n",
    "for key, values in dict_removed_single_entries.items():\n",
    "    # key = nafn ('ludvik')\n",
    "    for birthday, arrays in dict(values).items():\n",
    "        for item in arrays:\n",
    "            duplicate_ids_kept.append(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map teams table values to einstaklingsID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if two names are the same person\n",
    "dict_duplicate_compare_team_members = defaultdict(dict)\n",
    "for index, row in lidsmenn.iterrows():\n",
    "    ids = row[\"EinstID\"]\n",
    "    if ids in duplicate_ids_kept:\n",
    "        # now we only view ids that exist for duplicated people\n",
    "        #print(ids)\n",
    "        if ids in dict_duplicate_compare_team_members.keys():\n",
    "            dict_duplicate_compare_team_members[ids].append(row.values)\n",
    "        else:\n",
    "            dict_duplicate_compare_team_members[ids] = [row.values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map one step down (name->birthday -> value) now (name+birthday -> value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name_entries = {}\n",
    "for key, value in dict_removed_single_entries.items():\n",
    "    #get key and arrays for each person\n",
    "    for birthday, arrays in dict(value).items():\n",
    "        #get each array for person\n",
    "        #print(\"KEY: \" + key + \" BIRTHDAY: \" + birthday)\n",
    "        new_key = key +\"-\"+ birthday\n",
    "        for item in arrays:\n",
    "            if new_key in dict_name_entries.keys():\n",
    "                dict_name_entries[new_key].append(item[0])\n",
    "            else:\n",
    "                dict_name_entries[new_key] = [item[0]]\n",
    "#dict_name_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EinstaklingsID+birthday connected to all his data from teams table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_einstaklingar_teammember_info = {}\n",
    "for key, value in dict_name_entries.items():\n",
    "    #print(\"<key>\" + str(key) + \" <value> \" + str(value))\n",
    "    for item in value:\n",
    "        #print(item)\n",
    "        if item in dict_duplicate_compare_team_members.keys():\n",
    "            #print(\"<key>\" + str(key) + \" <item> \" + str(item))\n",
    "            for compare_arrays in dict_duplicate_compare_team_members[item]:\n",
    "                mot_id = compare_arrays[0]\n",
    "                lid_id = compare_arrays[1]\n",
    "                player_id = compare_arrays[2]\n",
    "                date_played = compare_arrays[3].split()[0]\n",
    "                \n",
    "                temp = (str(compare_arrays[3]) + \" \" + str(player_id) + \" \" + str(lid_id))   \n",
    "                if key in dict_einstaklingar_teammember_info.keys():\n",
    "                    dict_einstaklingar_teammember_info[key].append(temp)\n",
    "                else:\n",
    "                    dict_einstaklingar_teammember_info[key] = [temp]\n",
    "        #print(\"xxxxxxxxxxxx\")\n",
    "#dict_einstaklingar_teammember_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find if a potential duplicated person played two games at the same time in different teams (then he is not a duplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Time Diff: 22:54:28\n",
      "Key: ana-1972-01-29\n",
      "Row 1: \n",
      "2012-04-11 13:50:36.313 2065 4155\n",
      "Row 2: \n",
      "2012-04-11 22:54:28.157 2065 1840\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 14:03:03\n",
      "Key: anna-1972-05-30\n",
      "Row 1: \n",
      "2005-03-29 13:56:01.373 1766 2226\n",
      "Row 2: \n",
      "2005-03-29 14:03:03.560 1768 2227\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 18:01:22\n",
      "Key: berglind-1975-10-31\n",
      "Row 1: \n",
      "2010-04-15 14:45:54.687 3104 4448\n",
      "Row 2: \n",
      "2010-04-15 18:01:22.030 3119 4450\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 11:09:30\n",
      "Key: bragi-1931-04-26\n",
      "Row 1: \n",
      "2007-01-05 11:01:51.687 1852 132\n",
      "Row 2: \n",
      "2007-01-05 11:09:30.640 541 2873\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 17:48:28\n",
      "Key: bryndis-1976-08-13\n",
      "Row 1: \n",
      "2010-04-15 14:53:52.967 3107 4448\n",
      "Row 2: \n",
      "2010-04-15 17:48:28.827 3113 4449\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 17:49:36\n",
      "Key: charlotta-1978-02-21\n",
      "Row 1: \n",
      "2010-04-15 14:46:45.247 3105 4448\n",
      "Row 2: \n",
      "2010-04-15 17:49:36.890 3114 4449\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 22:10:12\n",
      "Key: erla-1976-02-10\n",
      "Row 1: \n",
      "2014-04-01 22:03:38.327 3654 6286\n",
      "Row 2: \n",
      "2014-04-01 22:10:12.233 3654 6285\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 13:09:44\n",
      "Key: eyglo-1977-05-14\n",
      "Row 1: \n",
      "2013-10-18 10:54:49.060 4258 2504\n",
      "Row 2: \n",
      "2013-10-18 13:09:44.120 1620 2691\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 21:00:46\n",
      "Key: fridrik-1950-12-23\n",
      "Row 1: \n",
      "2000-04-29 20:50:37.857 209 60\n",
      "Row 2: \n",
      "2000-04-29 21:00:46.300 507 61\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 22:19:56\n",
      "Key: fridrik-1950-12-23\n",
      "Row 1: \n",
      "2002-04-08 22:06:25.513 507 61\n",
      "Row 2: \n",
      "2002-04-08 22:19:56.107 209 60\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 23:16:41\n",
      "Key: gaetan-1982-03-16\n",
      "Row 1: \n",
      "2013-10-24 23:15:55.993 4268 1830\n",
      "Row 2: \n",
      "2013-10-24 23:16:41.120 4276 3040\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 18:02:30\n",
      "Key: gudlaug-1978-07-20\n",
      "Row 1: \n",
      "2010-04-15 16:11:24.700 3111 4448\n",
      "Row 2: \n",
      "2010-04-15 18:02:30.233 3120 4450\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 22:55:06\n",
      "Key: gudrun-1978-05-29\n",
      "Row 1: \n",
      "2012-04-11 13:51:55.530 3224 4155\n",
      "Row 2: \n",
      "2012-04-11 22:55:06.780 3098 1840\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 11:48:58\n",
      "Key: harald-1960-11-18\n",
      "Row 1: \n",
      "2001-04-10 10:28:14.137 393 71\n",
      "Row 2: \n",
      "2001-04-10 11:48:58.813 393 72\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 11:18:05\n",
      "Key: heidi-1968-08-09\n",
      "Row 1: \n",
      "2014-08-13 10:45:27.647 2044 4173\n",
      "Row 2: \n",
      "2014-08-13 11:18:05.767 2192 6011\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 13:09:59\n",
      "Key: helga-1964-03-07\n",
      "Row 1: \n",
      "2013-10-18 10:36:16.310 1358 2504\n",
      "Row 2: \n",
      "2013-10-18 13:09:59.540 1358 2691\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 18:04:19\n",
      "Key: hjordis-1965-12-09\n",
      "Row 1: \n",
      "2010-04-15 15:06:52.593 3108 4448\n",
      "Row 2: \n",
      "2010-04-15 18:04:19.640 3121 4450\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 14:47:45\n",
      "Key: hugrun-1971-07-16\n",
      "Row 1: \n",
      "2010-03-15 08:56:44.437 842 140\n",
      "Row 2: \n",
      "2010-03-15 14:47:45.217 842 1049\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 17:20:32\n",
      "Key: hulda-1962-02-24\n",
      "Row 1: \n",
      "2005-03-30 17:18:35.810 1772 2227\n",
      "Row 2: \n",
      "2005-03-30 17:20:32.980 1773 2226\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 14:15:56\n",
      "Key: jounes-1977-03-21\n",
      "Row 1: \n",
      "2003-10-09 14:08:51.403 1231 1019\n",
      "Row 2: \n",
      "2003-10-09 14:15:56.310 1049 1023\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 15:50:23\n",
      "Key: kristrun-1969-09-29\n",
      "Row 1: \n",
      "2008-04-07 15:31:32.937 1678 2071\n",
      "Row 2: \n",
      "2008-04-07 15:50:23.060 1830 3261\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 13:11:29\n",
      "Key: linda-1977-11-19\n",
      "Row 1: \n",
      "2013-10-18 10:36:02.623 1596 2504\n",
      "Row 2: \n",
      "2013-10-18 13:11:29.130 1596 2692\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 21:08:30\n",
      "Key: magnus-1950-02-23\n",
      "Row 1: \n",
      "2000-04-29 20:54:50.007 419 60\n",
      "Row 2: \n",
      "2000-04-29 21:08:30.990 489 61\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 22:24:49\n",
      "Key: magnus-1950-02-23\n",
      "Row 1: \n",
      "2002-04-08 22:05:20.090 489 61\n",
      "Row 2: \n",
      "2002-04-08 22:24:49.840 419 60\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 22:51:24\n",
      "Key: olafur-1959-05-06\n",
      "Row 1: \n",
      "2005-04-18 22:40:16.903 1497 1689\n",
      "Row 2: \n",
      "2005-04-18 22:51:24.403 722 38\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 09:27:52\n",
      "Key: olafur-1959-05-06\n",
      "Row 1: \n",
      "2008-04-24 09:15:58.937 722 2980\n",
      "Row 2: \n",
      "2008-04-24 09:27:52.903 1497 38\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 21:00:41\n",
      "Key: olafur-1959-05-06\n",
      "Row 1: \n",
      "2010-05-05 20:55:47.937 1497 2980\n",
      "Row 2: \n",
      "2010-05-05 21:00:41.653 722 1689\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 23:25:23\n",
      "Key: samuel-1975-07-12\n",
      "Row 1: \n",
      "2014-03-19 13:03:04.100 2726 6287\n",
      "Row 2: \n",
      "2014-03-19 23:25:23.967 2726 3500\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 22:57:40\n",
      "Key: sigurbjorg-1980-06-30\n",
      "Row 1: \n",
      "2012-04-11 13:51:32.797 3227 4155\n",
      "Row 2: \n",
      "2012-04-11 22:57:40.810 3227 1840\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 10:32:51\n",
      "Key: skjoldur-1947-03-09\n",
      "Row 1: \n",
      "2001-04-10 09:23:47.367 178 71\n",
      "Row 2: \n",
      "2001-04-10 10:32:51.187 178 27\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 19:14:21\n",
      "Key: skjoldur-1947-03-09\n",
      "Row 1: \n",
      "2001-12-11 19:03:20.107 178 72\n",
      "Row 2: \n",
      "2001-12-11 19:14:21.047 750 27\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 18:16:40\n",
      "Key: unnur-1957-03-25\n",
      "Row 1: \n",
      "2001-04-13 14:59:24.037 284 54\n",
      "Row 2: \n",
      "2001-04-13 18:16:40.377 780 126\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 14:48:47\n",
      "Key: valgeir-1967-10-31\n",
      "Row 1: \n",
      "2002-05-09 14:41:40.857 1205 671\n",
      "Row 2: \n",
      "2002-05-09 14:48:47.640 1215 670\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 13:11:49\n",
      "Key: thorgerdur-1967-11-12\n",
      "Row 1: \n",
      "2013-10-18 10:35:33.030 1646 2504\n",
      "Row 2: \n",
      "2013-10-18 13:11:49.973 1646 2692\n",
      "END OF THESE ROWS\n",
      "=======================\n",
      "Time Diff: 18:08:01\n",
      "Key: thorgerdur-1978-10-07\n",
      "Row 1: \n",
      "2010-04-15 14:50:45.000 3106 4448\n",
      "Row 2: \n",
      "2010-04-15 18:08:01.047 3125 4450\n",
      "END OF THESE ROWS\n"
     ]
    }
   ],
   "source": [
    "not_the_same_person = {}\n",
    "\n",
    "def getTimeDifferenceFromNow(TimeStart, TimeEnd):\n",
    "    timeDiff = TimeEnd - TimeStart\n",
    "    return timeDiff.total_seconds() / 60\n",
    "\n",
    "def find_duplicates(key, nums):\n",
    "    num_set = set()\n",
    "    duplicates = set()\n",
    "    no_duplicate = -1\n",
    "    sorted_nums = sorted(nums)\n",
    "    \n",
    "    for i in range(len(sorted_nums)):\n",
    "        for j in range(i+1, len(sorted_nums)):\n",
    "            \n",
    "            # team one split\n",
    "            sort_1 = sorted_nums[i].split()\n",
    "            date_1 = sort_1[0]\n",
    "            einstaklings_id_1 = sort_1[2]\n",
    "            team_id_1 = sort_1[3]\n",
    "            \n",
    "            #hour minutes seconds\n",
    "            time_1 = sort_1[1]\n",
    "            time_1_split = time_1.split(\":\")\n",
    "            time_1_hour = time_1_split[0]\n",
    "            time_1_min = time_1_split[1]\n",
    "            time_1_sec_split = time_1_split[2].split(\".\")\n",
    "            time_1_sec = time_1_sec_split[0]\n",
    "            time_1_micro = time_1_sec_split[1]\n",
    "            #print(time_1_micro)\n",
    "            time_1_final = datetime.time(int(time_1_hour), int(time_1_min), int(time_1_sec))\n",
    "            \n",
    "            \n",
    "            # team two split\n",
    "            sort_2 = sorted_nums[j].split()\n",
    "            date_2 = sort_2[0]\n",
    "            einstaklings_id_2 = sort_2[2]\n",
    "            team_id_2 = sort_2[3]\n",
    "            \n",
    "            #hour minutes seconds\n",
    "            time_2 = sort_2[1]\n",
    "            time_2_split = time_2.split(\":\")\n",
    "            time_2_hour = time_2_split[0]\n",
    "            time_2_min = time_2_split[1]\n",
    "            time_2_sec_split = time_2_split[2].split(\".\")\n",
    "            time_2_sec = time_2_sec_split[0]\n",
    "            time_2_micro = time_2_sec_split[1]\n",
    "            time_2_final = datetime.time(int(time_2_hour), int(time_2_min), int(time_2_sec))\n",
    "            \n",
    "            \n",
    "            #time_diff = (time_2_final - time_1_final).time()\n",
    "            \n",
    "            if((date_1 == date_2) and (team_id_1 != team_id_2)):\n",
    "                print(\"=======================\")\n",
    "                print(\"Time Diff: \" + str(time_2_final))\n",
    "                #print(\"Time diff: \" + time_diff)\n",
    "                print(\"Key: \" + key)\n",
    "                print(\"Row 1: \")\n",
    "                print(sorted_nums[i])\n",
    "                print(\"Row 2: \")\n",
    "                print(sorted_nums[j]);\n",
    "                print(\"END OF THESE ROWS\")\n",
    "                \n",
    "    \n",
    "for key, value in dict_einstaklingar_teammember_info.items():\n",
    "    #print(value)\n",
    "    find_duplicates(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverted_back_to_dict = dict(duplicate_dict)\n",
    "#reverted_back_to_dict\n",
    "#file_path = \"json/einstaklingar_map.txt\" ## your path variable\n",
    "#duplicate_dict_json = json.dump(duplicate_dict, codecs.open(file_path, 'w', encoding='utf-8'), separators=(';', ':'), sort_keys=True, indent=4) ### this saves the array in .json format\n",
    "#json_obj = json.dumps(duplicate_dict, indent = 4)\n",
    "#dumped = json.dumps(duplicate_dict, cls=NumpyEncoder)\n",
    "#dumped\n",
    "#pd.DataFrame(reverted_back_to_dict).to_csv(file_path, encoding='utf-8-sig')\n",
    "#duplicate_dict_json = json.dump(reverted_back_to_dict, codecs.open(file_path, 'w', encoding='utf-8-sig'))\n",
    "\n",
    "#json = json.dumps(reverted_back_to_dict)\n",
    "#f = open(file_path,\"w\")\n",
    "#f.write(str(reverted_back_to_dict))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================================================================\n",
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL STEP (run after everything is done):\n",
    "\n",
    "#duplicated people put into it's own csv to be browsed later\n",
    "pd.DataFrame(duplicated_einstaklingar).to_csv(\"csv/new/duplicated-einstaklingar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(duplicate_dict).to_csv(\"json/duplicate-map.json\", encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "#save as new csv inside csv/new\n",
    "pd.DataFrame(domarar).to_csv(\"csv/new/blak-domarar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(einstaklingar).to_csv(\"csv/new/blak-einstaklingar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(forsvarsmenn).to_csv(\"csv/new/blak-forsvarsmenn.csv.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lid).to_csv(\"csv/new/blak-lid.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidimoti).to_csv(\"csv/new/blak-lidimoti.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsmenn).to_csv(\"csv/new/blak-lidsmenn.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsstjorar).to_csv(\"csv/new/blak-lidsstjorar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(mot).to_csv(\"csv/new/blak-mot.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(thjalfarar).to_csv(\"csv/new/blak-thjalfarar.csv\", encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
