{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import codecs, json\n",
    "import unicodedata\n",
    "# pip install Unidecode  <OR> conda install Unidecode\n",
    "import unidecode\n",
    "import collections\n",
    "import datetime\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables and what they mean\n",
    "#   duplicate_dict = all duplicates\n",
    "#   dict_removed_single_entries = all duplicates but removed single entries\n",
    "#   duplicate_ids_kept = array of all ids from dict_removed_single_entries\n",
    "#   dict_duplicate_compare_team_members = map values from teams to einstaklingsid\n",
    "#   dict_name_entries = map values from member down one step (name->birthday->values) now (name+birthday->values)\n",
    "#   dict_einstaklingar_teammember_info = map teammember values to correct key in einstaklingsid\n",
    "#   not_the_same_person = when two players are playing in different teams at the same time then they clearly are not the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all csv files\n",
    "domarar = pd.read_csv('csv/blak-domarar.csv', sep=';', header=0)\n",
    "einstaklingar = pd.read_csv('csv/blak-einstaklingar.csv', sep=';', header=0)\n",
    "forsvarsmenn = pd.read_csv('csv/blak-forsvarsmenn.csv', sep=';', header=0)\n",
    "lid = pd.read_csv('csv/blak-lid.csv', sep=';', header=0)\n",
    "lidimoti = pd.read_csv('csv/blak-lidimoti.csv', sep=';', header=0)\n",
    "lidsmenn = pd.read_csv('csv/blak-lidsmenn.csv', sep=';', header=0)\n",
    "lidsstjorar = pd.read_csv('csv/blak-lidsstjorar.csv', sep=';', header=0)\n",
    "thjalfarar = pd.read_csv('csv/blak-thjalfarar.csv', sep=';', header=0)\n",
    "mot = pd.read_csv('csv/blak-mot.csv', sep=';', header=0)\n",
    "\n",
    "# drop all SyndarLids with an ID (SyndarlidID)\n",
    "# (the reason for not dropping using SyndarLid is because I don't trust that column to be inserted correctly with [0,1])\n",
    "lid = lid[lid['SyndarlidID'].isna()]\n",
    "# then dropping those two columns because we don't want virtual teams\n",
    "lid = lid.drop(columns=['SyndarLid', 'SyndarlidID'])\n",
    "\n",
    "# All duplicated birthdays\n",
    "duplicated_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Nafn', 'Fdagur', 'Kyn'], keep=False)]\n",
    "duplicated_fdagur_kyn_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Fdagur', 'Kyn'], keep=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get RadNumbers From TeamNames to the Right column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid_updated = lid\n",
    "def correctRadNumbersFromEntries(array):\n",
    "    for key, values in array.iterrows():\n",
    "        nafn = values[\"Nafn\"]\n",
    "        nafn_split = nafn.split()\n",
    "        nafn_last_letter = nafn_split[-1]\n",
    "        \n",
    "        radNumber = values['Radnumer']\n",
    "        lid_id = values['LidID']\n",
    "        \n",
    "        if(nafn_split[0] != nafn_last_letter):\n",
    "            if(len(nafn_last_letter) < 2):\n",
    "                if((nafn_last_letter.islower()) or (nafn_last_letter.isnumeric())):\n",
    "                    new_nafn = ' '.join(nafn_split[:-1])\n",
    "                    #clear on extra spaces in name\n",
    "                    new_nafn_joined = re.sub(\"\\s+\", \" \", new_nafn)\n",
    "                    \n",
    "                    lid_updated.at[key,'Radnumer'] = nafn_last_letter\n",
    "                    lid_updated.at[key,'Nafn'] = new_nafn_joined\n",
    "        \n",
    "# call def\n",
    "correctRadNumbersFromEntries(lid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lid_updated['Nafn'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lid['Nafn'].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter duplicates by name and birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all entries that have duplicated birthdays, then filter that to first_name->birthday-><people entries>\n",
    "duplicate_dict = defaultdict(dict)\n",
    "\n",
    "def create_duplicated_entries(duplication_array):\n",
    "    for index, row in duplication_array.iterrows():\n",
    "        full_name = row['Nafn']\n",
    "        first_name = full_name.split()[0]\n",
    "        last_name = full_name.split()[::-1][0]\n",
    "\n",
    "        # make name lowercase\n",
    "        first_name_lowercase = first_name.lower()\n",
    "        last_name_lowercase = last_name.lower()\n",
    "\n",
    "        # encode icelandic letters to english\n",
    "        first_name_to_english = unidecode.unidecode(first_name_lowercase)\n",
    "        last_name_to_english =  unidecode.unidecode(last_name_lowercase)\n",
    "\n",
    "        # split birthday into year month and day and ignore second part (sec, min, hour)\n",
    "        if (last_name_to_english != first_name_to_english):\n",
    "            Fdagur_date = row['Fdagur'].split()[0] + \"+\" + last_name_to_english\n",
    "        else:\n",
    "            Fdagur_date = row['Fdagur'].split()[0] + \"+\" + \"<MISSING>\"\n",
    "\n",
    "        if first_name_to_english in duplicate_dict.keys():\n",
    "            if Fdagur_date in duplicate_dict[first_name_to_english].keys():\n",
    "                #if first name and Fdagur (birthday) exist in dict then append to that key (birthday)\n",
    "                duplicate_dict[first_name_to_english][Fdagur_date].append(row.values)\n",
    "            else:\n",
    "                #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "                duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "        else:\n",
    "            #if Fdagur (birthday) does not exist in dict\n",
    "            duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "            \n",
    "# call def\n",
    "create_duplicated_entries(duplicated_fdagur_kyn_einstaklingar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all single birthday entries (since that is not a duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all single birthday entries that are not duplicates\n",
    "dict_removed_single_entries = defaultdict(dict)\n",
    "\n",
    "def remove_single_entries(duplication_array):\n",
    "    for key, values in duplication_array.items():\n",
    "        # key = nafn ('ludvik')\n",
    "        for birthday, arrays in dict(values).items():\n",
    "            # only get duplicates that there exists 2 or more entries for a birthday\n",
    "            if(len(arrays) > 1):\n",
    "                # used for when joining teams table\n",
    "                if key in dict_removed_single_entries.keys():\n",
    "                    if birthday in dict_removed_single_entries[key].keys():\n",
    "                        dict_removed_single_entries[key][birthday].append(arrays)\n",
    "                    else:\n",
    "                        #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "                        dict_removed_single_entries[key][birthday] = arrays\n",
    "                else:\n",
    "                    dict_removed_single_entries[key][birthday] = arrays\n",
    "\n",
    "# call def\n",
    "remove_single_entries(duplicate_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map (name+birthday+lastname-> unique id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name_entries = {}\n",
    "\n",
    "def identifier_map_unique_ids(duplication_array):\n",
    "    for key, value in duplication_array.items():\n",
    "        #get key and arrays for each person\n",
    "        for birthday, arrays in dict(value).items():\n",
    "            #get each array for person\n",
    "            new_key = key +\"+\"+ birthday\n",
    "            for item in arrays:\n",
    "                if new_key in dict_name_entries.keys():\n",
    "                    dict_name_entries[new_key].append(item[0])\n",
    "                else:\n",
    "                    dict_name_entries[new_key] = [item[0]]\n",
    "# call def\n",
    "identifier_map_unique_ids(dict_removed_single_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all ids that exists in duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ids in dict_removed_single_entries\n",
    "duplicate_ids_kept = []\n",
    "def get_duplication_keys(duplication_array):\n",
    "    for key, values in duplication_array.items():\n",
    "        # key = nafn ('ludvik')\n",
    "        for birthday, arrays in dict(values).items():\n",
    "            for item in arrays:\n",
    "                duplicate_ids_kept.append(item[0])\n",
    "\n",
    "# call def\n",
    "get_duplication_keys(dict_removed_single_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Lid table values to einstaklingsID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if two names are the same person\n",
    "dict_duplicate_compare_team_members = defaultdict(dict)\n",
    "\n",
    "def team_member_entries(duplication_array):\n",
    "    for index, row in duplication_array.iterrows():\n",
    "        ids = row[\"EinstID\"]\n",
    "        if ids in duplicate_ids_kept:\n",
    "            # now we only view ids that exist for duplicated people\n",
    "            #print(ids)\n",
    "            if ids in dict_duplicate_compare_team_members.keys():\n",
    "                dict_duplicate_compare_team_members[ids].append(row.values)\n",
    "            else:\n",
    "                dict_duplicate_compare_team_members[ids] = [row.values]\n",
    "\n",
    "# call def\n",
    "team_member_entries(lidsmenn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EinstaklingsID connect to his data in teams table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_einstaklingar_teammember_info = {}\n",
    "\n",
    "def connect_members_to_team_data(duplication_array):\n",
    "    for key, value in duplication_array.items():\n",
    "        #print(\"<key>\" + str(key) + \" <value> \" + str(value))\n",
    "        for item in value:\n",
    "            #print(item)\n",
    "            if item in dict_duplicate_compare_team_members.keys():\n",
    "                #print(\"<key>\" + str(key) + \" <item> \" + str(item))\n",
    "                for compare_arrays in dict_duplicate_compare_team_members[item]:\n",
    "                    mot_id = compare_arrays[0]\n",
    "                    lid_id = compare_arrays[1]\n",
    "                    player_id = compare_arrays[2]\n",
    "                    date = compare_arrays[3]\n",
    "                    date_played = compare_arrays[3].split()[0]\n",
    "\n",
    "                    temp = (str(date) + \" \" + str(mot_id) + \" \" + str(lid_id) + \" \" + str(player_id))   \n",
    "                    if key in dict_einstaklingar_teammember_info.keys():\n",
    "                        dict_einstaklingar_teammember_info[key].append(temp)\n",
    "                    else:\n",
    "                        dict_einstaklingar_teammember_info[key] = [temp]\n",
    "\n",
    "# call def\n",
    "connect_members_to_team_data(dict_name_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find if a potential duplicated person played two games at the same time in different teams (then he is not a duplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_the_same_person = {}\n",
    "most_likely_same_person = {}\n",
    "\n",
    "def find_duplicates(key, nums):\n",
    "    num_set = set()\n",
    "    duplicates = set()\n",
    "    no_duplicate = -1\n",
    "    sorted_nums = sorted(nums)\n",
    "    last_array_entry = \"\"\n",
    "    for i in range(len(sorted_nums)):\n",
    "        for j in range(i+1, len(sorted_nums)):\n",
    "            \n",
    "            # team one split\n",
    "            #(str(date) + \" \" + str(mot_id) + \" \" + str(lid_id) +  str(player_id))   \n",
    "            \n",
    "            sort_1 = sorted_nums[i].split()\n",
    "            date_1 = sort_1[0]\n",
    "            mot_id_1 = sort_1[2]\n",
    "            team_id_1 = sort_1[3]\n",
    "            einstaklings_id_1 = sort_1[4]\n",
    "            date_time_str_1 = sort_1[0]+\" \"+sort_1[1]\n",
    "            date_time_obj_1 = datetime.datetime.strptime(date_time_str_1, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            \n",
    "            # team two split\n",
    "            sort_2 = sorted_nums[j].split()\n",
    "            date_2 = sort_2[0]\n",
    "            mot_id_1 = sort_2[2]\n",
    "            team_id_2 = sort_2[3]\n",
    "            einstaklings_id_2 = sort_2[4]    \n",
    "            date_time_str_2 = sort_2[0]+\" \"+sort_2[1]\n",
    "            date_time_obj_2 = datetime.datetime.strptime(date_time_str_2, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            \n",
    "            # time difference between these two entries\n",
    "            time_diff = (date_time_obj_2 - date_time_obj_1).total_seconds()/60\n",
    "            \n",
    "            match_length = 60\n",
    "            # There exist two record for erla with the same einstaklingsid but different teams (6286 and 6285)\n",
    "            # played 6.5 minutes apart\n",
    "            # TODO: figure out how to handle that\n",
    "            if((date_1 == date_2) and (team_id_1 != team_id_2) and (time_diff < match_length) and (einstaklings_id_1 != einstaklings_id_2)):\n",
    "                #print(\"=======================\")\n",
    "                #print(\"SAME ID\")\n",
    "                #print(\"Time diff: \" + str(time_diff))\n",
    "                #print(\"Key: \" + key)\n",
    "                #print(\"Row 1: \" + sorted_nums[i])\n",
    "                #print(\"Row 2: \" + sorted_nums[j])\n",
    "                combined = [sorted_nums[i], sorted_nums[j]]\n",
    "                if key in not_the_same_person.keys():\n",
    "                    not_the_same_person[key].append(combined)\n",
    "                else:\n",
    "                    not_the_same_person[key] = [combined]\n",
    "                    \n",
    "            else:\n",
    "                if last_array_entry != sorted_nums[i]:\n",
    "                    # Here are all the potential duplicates left after filtering\n",
    "                    #combined = [sorted_nums[i], sorted_nums[j]]\n",
    "                    if key in most_likely_same_person.keys():\n",
    "                        most_likely_same_person[key].append(sorted_nums[i])\n",
    "                    else:\n",
    "                        most_likely_same_person[key] = [sorted_nums[i]]\n",
    "                last_array_entry = sorted_nums[i]\n",
    "\n",
    "for key, value in dict_einstaklingar_teammember_info.items():\n",
    "    find_duplicates(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_likely_same_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not_the_same_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All ids that should be able to be merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list = {}\n",
    "\n",
    "def people_abled_to_merge(duplication_array):\n",
    "    temp_arr = []\n",
    "    def merged_ids(key, nums):\n",
    "        for compare_arrays in nums:\n",
    "\n",
    "            sort_1 = compare_arrays.split()\n",
    "            einstaklings_id_1 = sort_1[4]\n",
    "\n",
    "            if key in merged_list.keys():\n",
    "                if einstaklings_id_1 not in temp_arr:\n",
    "                    #print(key)\n",
    "                    #print(einstaklings_id_1)\n",
    "                    merged_list[key].append(einstaklings_id_1)\n",
    "                    temp_arr.append(einstaklings_id_1)\n",
    "            else:\n",
    "                merged_list[key] = [einstaklings_id_1]\n",
    "                temp_arr.append(einstaklings_id_1)\n",
    "\n",
    "    for key, value in duplication_array.items():\n",
    "        merged_ids(key, value)\n",
    "        temp_arr = []\n",
    "\n",
    "# call def\n",
    "people_abled_to_merge(most_likely_same_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR NAMES WITH MISSING> FIND IF THERE EXISTS A CLOSE ENOUGH MATCH FROM THAT DAY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POTENTIAL MATHCES FOR MISSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================================================================\n",
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverted_back_to_dict = dict(duplicate_dict)\n",
    "#reverted_back_to_dict\n",
    "#file_path = \"json/einstaklingar_map.txt\" ## your path variable\n",
    "#duplicate_dict_json = json.dump(duplicate_dict, codecs.open(file_path, 'w', encoding='utf-8'), separators=(';', ':'), sort_keys=True, indent=4) ### this saves the array in .json format\n",
    "#json_obj = json.dumps(duplicate_dict, indent = 4)\n",
    "#dumped = json.dumps(duplicate_dict, cls=NumpyEncoder)\n",
    "#dumped\n",
    "#pd.DataFrame(reverted_back_to_dict).to_csv(file_path, encoding='utf-8-sig')\n",
    "#duplicate_dict_json = json.dump(reverted_back_to_dict, codecs.open(file_path, 'w', encoding='utf-8-sig'))\n",
    "\n",
    "#json = json.dumps(reverted_back_to_dict)\n",
    "#f = open(file_path,\"w\")\n",
    "#f.write(str(reverted_back_to_dict))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================================================================\n",
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL STEP (run after everything is done):\n",
    "\n",
    "#duplicated people put into it's own csv to be browsed later\n",
    "pd.DataFrame(duplicated_einstaklingar).to_csv(\"csv/new/duplicated-einstaklingar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(duplicate_dict).to_csv(\"json/duplicate-map.json\", encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "#save as new csv inside csv/new\n",
    "pd.DataFrame(domarar).to_csv(\"csv/new/blak-domarar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(einstaklingar).to_csv(\"csv/new/blak-einstaklingar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(forsvarsmenn).to_csv(\"csv/new/blak-forsvarsmenn.csv.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidi).to_csv(\"csv/new/blak-lid.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidimoti).to_csv(\"csv/new/blak-lidimoti.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsmenn).to_csv(\"csv/new/blak-lidsmenn.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsstjorar).to_csv(\"csv/new/blak-lidsstjorar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(mot).to_csv(\"csv/new/blak-mot.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(thjalfarar).to_csv(\"csv/new/blak-thjalfarar.csv\", encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
